# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
new_observations = rexp(50, rate = max_likelihood_all$maximum)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
#new_observations = rexp(50, rate = max_likelihood_all$maximum)
new_observations = rexp(50, rate = 10)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
#new_observations = rexp(50, rate = max_likelihood_all$maximum)
new_observations = rexp(50, rate = 10)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
#new_observations = rexp(50, rate = max_likelihood_all$maximum)
new_observations = rexp(50, rate = 100)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
hist(machines, plot=TRUE, main="The original observations")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
new_observations = rexp(50, rate = max_likelihood_all$maximum)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
new_observations = rexp(50, rate = max_likelihood_all$maximum)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
new_observations = rexp(50, rate = max_likelihood_all$maximum)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
hist(machines, plot=TRUE, main="The original observations")
library(MASS)
# read data into dataframe
data = read.csv("tecator.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
# plot protein vs moisture fields
plot(data$Protein, data$Moisture, xlab="Protein", ylab="Moisture", main="Protein vs Moisture")
# data seems to be correlated in a linear relationship, i.e. as values of one variable rises so does the other at a similar ratio
# these data can therefore be described well by a linear model
# a probabilistic model for predicting moisture (M-hat) would therefore be a polynomial function of protein as:
# M-hat = w0 + w1x^1 + w2x^2 + ... + wix^i
# TODO: Why MSE instead of SSE?
# MSE = SSE/n-m where n=sample size, m=number of parameters (sum(w0..wi)).
# and also:
# MSE = 1/n * sum((Yi - Y-hat)^2)
# MSE will be more dependent on the number of parameters used?
# divide data into 50/50 training and validation sets
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
validation = data[-id,]
train_mse = numeric(6)
validation_mse = numeric(6)
# fit a linear model using i = 0, 1, .. 6
for (i in 1:6)
{
linear_model = lm(formula = Moisture ~ poly(Protein, i), data=train)
validation_predictions = predict(linear_model, validation)
train_predictions = predict(linear_model, train)
validation_mse[i] = mean((validation$Moisture - validation_predictions)^2)
train_mse[i] = mean((train$Moisture - train_predictions)^2)
}
# plotting we see that i = 1 gives the lowest error in the validation data, while the error
# goes down as i increases in the training data. since the model was fitted to the training data,
# more parameters will lead to lower error in the training data but will create an overfitted model
i_values = c(1:6)
y_limits = c(min(train_mse, validation_mse), max(train_mse, validation_mse))
plot(i_values, main="MSE", ylim=y_limits, xlab="i", ylab="MSE")
lines(validation_mse, col="green")
lines(train_mse, col="red")
text(locator(), labels = c("Validation MSE", "Train MSE"))
# perform a variable selection from a linear model in which Fat is a response and
# Channel1-Channel100 are predictors
# useful: http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf
linear_model = lm(Fat ~ . -Protein -Moisture, data=train)
step_aic = stepAIC(linear_model, direction="both")
summary(step_aic)
library(MASS)
# read data into dataframe
data = read.csv("tecator.csv", header = TRUE, sep = ",", quote = "\"",
dec = ",", fill = TRUE, comment.char = "")
# plot protein vs moisture fields
plot(data$Protein, data$Moisture, xlab="Protein", ylab="Moisture", main="Protein vs Moisture")
# data seems to be correlated in a linear relationship, i.e. as values of one variable rises so does the other at a similar ratio
# these data can therefore be described well by a linear model
# a probabilistic model for predicting moisture (M-hat) would therefore be a polynomial function of protein as:
# M-hat = w0 + w1x^1 + w2x^2 + ... + wix^i
# TODO: Why MSE instead of SSE?
# MSE = SSE/n-m where n=sample size, m=number of parameters (sum(w0..wi)).
# and also:
# MSE = 1/n * sum((Yi - Y-hat)^2)
# MSE will be more dependent on the number of parameters used?
# divide data into 50/50 training and validation sets
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
validation = data[-id,]
train_mse = numeric(6)
validation_mse = numeric(6)
# fit a linear model using i = 0, 1, .. 6
for (i in 1:6)
{
linear_model = lm(formula = Moisture ~ poly(Protein, i), data=train)
validation_predictions = predict(linear_model, validation)
train_predictions = predict(linear_model, train)
validation_mse[i] = mean((validation$Moisture - validation_predictions)^2)
train_mse[i] = mean((train$Moisture - train_predictions)^2)
}
# plotting we see that i = 1 gives the lowest error in the validation data, while the error
# goes down as i increases in the training data. since the model was fitted to the training data,
# more parameters will lead to lower error in the training data but will create an overfitted model
i_values = c(1:6)
y_limits = c(min(train_mse, validation_mse), max(train_mse, validation_mse))
plot(i_values, main="MSE", ylim=y_limits, xlab="i", ylab="MSE")
lines(validation_mse, col="green")
lines(train_mse, col="red")
text(locator(), labels = c("Validation MSE", "Train MSE"))
# perform a variable selection from a linear model in which Fat is a response and
# Channel1-Channel100 are predictors
# useful: http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf
linear_model = lm(Fat ~ . -Protein -Moisture, data=train)
step_aic = stepAIC(linear_model, direction="both")
summary(step_aic)
new_observations = rexp(50, rate = 100)
# load data from csv and convert to matrix
# if csv is using comma separator of decimal points,
# change to data = "," in argument passed to read.csv()
data_csv = read.csv("machines.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
machines = as.matrix(data_csv)
# the probability function used to calculate the log-likelihood
probability=function(theta, data) {
return (theta * exp((-theta) * data))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using all observations
sum_log_likelihood=function(theta) {
return(sum(log(probability(theta, machines))))
}
# sum of log-likelihoods function, returns sum of the log-likelihoods using just the first 6 observations
sum_log_likelihood2=function(theta) {
return(sum(log(probability(theta, machines[1:6]))))
}
# prior probability for theta for the bayesian model
prior_probability=function(theta) {
return (10 * exp(-10 * theta))
}
# sum of log of posterior likelihoods
sum_log_posterior_likelihood=function(theta) {
return (sum(log(probability(theta, machines)*prior_probability(theta))))
}
# function needs to be vectorized to be accepted by curve.. obviously! (?)
f <- Vectorize(sum_log_likelihood)
# plot function for values of theta from 0 to 3
curve(f, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (all observations)")
# find the theta value that maximizes the sum of log-likelihoods function with all observations
max_likelihood_all = optimize(f, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using all observations is given by theta =", max_likelihood_all$maximum, "\n")
# repeat using only the first six observations in 'machines'
# (can't figure out how to pass several arguments when plotting with curve() function)
f2 <- Vectorize(sum_log_likelihood2)
curve(f2, from=0, to=3, xlab="Theta", ylab="Sum of log-likelihoods (first six observations)")
max_likelihood_first6 = optimize(f2, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood value using only first 6 observations is given by theta =", max_likelihood_first6$maximum, "\n")
# use the log-likelihood function where theta has a prior probability (bayesian model)
f3 <- Vectorize(sum_log_posterior_likelihood)
curve(f3, from=0, to=3, xlab="Theta", ylab="Sum of log of posterior likelihoods")
max_likelihood_bayesian = optimize(f3, interval=c(0, 3), maximum=TRUE)
cat("max log-likelihood in p(x, theta) gives optimal theta =", max_likelihood_bayesian$maximum, "\n")
# generate 50 new observations from the exponential distribution
#new_observations = rexp(50, rate = max_likelihood_all$maximum)
new_observations = rexp(50, rate = 100)
# plot histograms with the old and new observations
hist(machines, plot=TRUE, main="The original observations")
hist(new_observations, plot=TRUE, main="The new observations drawn from the exponential distribution")
library(MASS)
# read data into dataframe
data = read.csv("tecator.csv", header = TRUE, sep = ",", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
# plot protein vs moisture fields
plot(data$Protein, data$Moisture, xlab="Protein", ylab="Moisture", main="Protein vs Moisture")
# data seems to be correlated in a linear relationship, i.e. as values of one variable rises so does the other at a similar ratio
# these data can therefore be described well by a linear model
# a probabilistic model for predicting moisture (M-hat) would therefore be a polynomial function of protein as:
# M-hat = w0 + w1x^1 + w2x^2 + ... + wix^i
# TODO: Why MSE instead of SSE?
# MSE = SSE/n-m where n=sample size, m=number of parameters (sum(w0..wi)).
# and also:
# MSE = 1/n * sum((Yi - Y-hat)^2)
# MSE will be more dependent on the number of parameters used?
# divide data into 50/50 training and validation sets
set.seed(12345)
n = nrow(data)
id = sample(1:n, floor(n*0.5))
train = data[id,]
validation = data[-id,]
train_mse = numeric(6)
validation_mse = numeric(6)
# fit a linear model using i = 0, 1, .. 6
for (i in 1:6)
{
linear_model = lm(formula = Moisture ~ poly(Protein, i), data=train)
validation_predictions = predict(linear_model, validation)
train_predictions = predict(linear_model, train)
validation_mse[i] = mean((validation$Moisture - validation_predictions)^2)
train_mse[i] = mean((train$Moisture - train_predictions)^2)
}
# plotting we see that i = 1 gives the lowest error in the validation data, while the error
# goes down as i increases in the training data. since the model was fitted to the training data,
# more parameters will lead to lower error in the training data but will create an overfitted model
i_values = c(1:6)
y_limits = c(min(train_mse, validation_mse), max(train_mse, validation_mse))
plot(i_values, main="MSE", ylim=y_limits, xlab="i", ylab="MSE")
lines(validation_mse, col="green")
lines(train_mse, col="red")
text(locator(), labels = c("Validation MSE", "Train MSE"))
# perform a variable selection from a linear model in which Fat is a response and
# Channel1-Channel100 are predictors
# useful: http://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf
linear_model = lm(Fat ~ . -Protein -Moisture, data=train)
step_aic = stepAIC(linear_model, direction="backward")
summary(step_aic)
